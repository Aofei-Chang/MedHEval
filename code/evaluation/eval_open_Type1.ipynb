{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aefd978c-2851-4494-b3a4-74d0a30e36a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#set the openai env variable here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68665ab4-caca-40ea-9691-f092030bf7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_version = \"2024-05-01-preview\"\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "model_kwargs = dict(\n",
    "    model=\"gpt-4-128k\", \n",
    "    azure_endpoint=\"endpoint here\",\n",
    "    api_key=\"key here\",\n",
    "    api_version=api_version,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "gpt_model = AzureChatOpenAI(**model_kwargs, cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef6c387f-089e-40ca-bb7e-154b1340746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Iterator, List, Optional, Sequence, Union\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b95e38de-d856-4ff8-9cf3-6c5b74eb2343",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg_eval = \"\"\"\n",
    "You are provided with a JSON object of model answers from eleven models and a structured ground truth. The structured ground truth contains lists from three aspects: anatomy, symptom, and measurement. Your task is to evaluate each model's answer based on these aspects. Specifically, for each aspect, you need to count: The number of correctly recalled components (recall_number). The number of incorrect components that do not exist in the chest X-ray image and ground truth (wrong_number).\n",
    "\n",
    "The input format, a json object:\n",
    "{{\"structured_ground_truth\": a json object}, {\"models\":{\"model_1\": model_1_answer}, ...}}\n",
    "Your output format:\n",
    "{\n",
    "\"model_1\": {\"anatomy\": {recall_number\":int, \"wrong_number\":int}, \"symptom\":{...}, \"measurement\":{...}},\n",
    "\"model_2\": {\"anatomy\": {recall_number\":int, \"wrong_number\":int}, \"symptom\":{...}, \"measurement\":{...}},\n",
    "...\n",
    "}\n",
    "\n",
    "Instructions:\n",
    "- The recall should consider similar mentions, not just exact matches. Ensure you capture all possible correct components.\n",
    "- Do not count common anatomical structures in chest X-rays that are absent from the ground truth as incorrect components..\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c495eaa9-fa3f-4cf0-be92-62ac5fd95202",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Eval_single(BaseModel):\n",
    "    recall_number: int = Field(description=\"The number of correct components in the model responses.\")\n",
    "    wrong_number: int = Field(description=\"The number of wrong components in the model responses.\")\n",
    "    \n",
    "\n",
    "class Eval_Res(BaseModel):\n",
    "    anatomy: Eval_single = Field(description=\"The evaluation results of anatomy\")\n",
    "    measurement: Eval_single = Field(description=\"The evaluation results of measurement\")\n",
    "    symptom: Eval_single = Field(description=\"The evaluation results of symptom\")\n",
    "    \n",
    "class Eval_ResList(BaseModel):\n",
    "    GPT4o: Eval_Res = Field(description=\"The evaluation results of GPT4o.\", default=None)\n",
    "    CheXagent: Eval_Res = Field(description=\"The evaluation results of CheXagent.\")\n",
    "    LLaVA_med: Eval_Res = Field(description=\"The evaluation results of LLaVA-med.\")\n",
    "    LLaVA_med_15: Eval_Res = Field(description=\"The evaluation results of LLaVA-med_15.\")\n",
    "    LLaVA: Eval_Res = Field(description=\"The evaluation results of LLaVA.\")\n",
    "    LLaVA_13b: Eval_Res = Field(description=\"The evaluation results of LLaVA_13b.\")\n",
    "    LLM_CXR: Eval_Res = Field(description=\"The evaluation results of LLM-CXR.\")\n",
    "    Med_flamingo: Eval_Res = Field(description=\"The evaluation results of Med-flamingo.\")\n",
    "    MiniGPT4: Eval_Res = Field(description=\"The evaluation results of MiniGPT4.\")\n",
    "    XrayGPT: Eval_Res = Field(description=\"The evaluation results of XrayGPT.\")\n",
    "    RadFM: Eval_Res = Field(description=\"The evaluation results of RadFM.\")\n",
    "\n",
    "#Here we use the classic parser in LangChain: Pydantic, to ensure a strict parsing process\n",
    "eval_parser = PydanticOutputParser(pydantic_object=Eval_ResList)\n",
    "\n",
    "open_eval_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=system_msg_eval),\n",
    "    HumanMessagePromptTemplate(\n",
    "        prompt=PromptTemplate(\n",
    "            template=\"The structured ground_truth and models' model_response: {input_pairs}. Ensure that you read the model answers carefully and find the similar mentioned components as the recalled components. Your evaluation output in a JSON object format, without extra explanation:\",\n",
    "            input_variables=[\"input_pairs\"],\n",
    "            partial_variables={\"format_instructions\": eval_parser.get_format_instructions()},\n",
    "        )\n",
    "    ),\n",
    "])\n",
    "\n",
    "open_eval_chain = (\n",
    "    open_eval_prompt \n",
    "    | gpt_model \n",
    "    | eval_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "91449f9b-1757-4448-b859-adcec5d28122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model inference results\n",
    "def load_model_ans(model_name):\n",
    "    ans_path = f\"/path/to/model/answer/{model_name}/mimic_open_res.jsonl\"\n",
    "    with open(ans_path, \"r\") as f:\n",
    "        results = [json.loads(line) for line in f]\n",
    "    formatted_results = []\n",
    "    id_results_dict = dict()\n",
    "    for i in results:\n",
    "        _id = i[\"question_id\"]\n",
    "        new_dict = dict()\n",
    "        new_dict[\"structured_ground_truth\"] = i[\"structured_answer\"]\n",
    "        new_dict[\"model_answer\"] = i[\"model_answer\"]\n",
    "        id_results_dict[_id] = new_dict\n",
    "        formatted_results.append(new_dict)\n",
    "    return formatted_results, id_results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1e790c48-4ea1-4343-bf2e-12c6f1814a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatted_results[:5]\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "30414cf7-561b-4f78-b6f3-f9287dad8234",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_res_dict = dict()\n",
    "model_names = [\"LLM-CXR\", \"GPT4o\", \"llava-v1.6\", \"llava-v1.6-13b\", \"RadFM\", \"XrayGPT\", \"MiniGPT4\", \"Med-flamingo\", \"LLaVA-med-1.5\", \"LLaVA-med\", \"CheXagent\"]\n",
    "len(model_names)\n",
    "for model_name in model_names:\n",
    "    model_name_res_dict[model_name] = load_model_ans(model_name)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3a0b74e1-eff8-4ad0-8504-e5974ed08e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_evals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b0c3d2c-e934-4e65-9b64-2d9e061a690f",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-08-19T19:56:35.591858700Z",
     "start_time": "2024-08-19T19:56:35.586632300Z"
    }
   },
   "outputs": [],
   "source": [
    "question_ids = list(model_name_res_dict[\"LLM-CXR\"].keys())\n",
    "# question_ids\n",
    "for _id in tqdm(question_ids):\n",
    "    gt = model_name_res_dict[\"LLM-CXR\"][_id][\"structured_ground_truth\"]\n",
    "    try:\n",
    "        models_res = {\n",
    "            \"CheXagent\":model_name_res_dict[\"CheXagent\"][_id][\"model_answer\"], \n",
    "            \"LLaVA_med\":model_name_res_dict[\"LLaVA-med\"][_id][\"model_answer\"], \n",
    "            \"LLaVA_med_15\":model_name_res_dict[\"LLaVA-med-1.5\"][_id][\"model_answer\"], \n",
    "            \"LLaVA\":model_name_res_dict[\"llava-v1.6\"][_id][\"model_answer\"], \n",
    "            \"LLaVA_13b\":model_name_res_dict[\"llava-v1.6-13b\"][_id][\"model_answer\"], \n",
    "            \"LLM_CXR\":model_name_res_dict[\"LLM-CXR\"][_id][\"model_answer\"], \n",
    "            \"Med_flamingo\":model_name_res_dict[\"Med-flamingo\"][_id][\"model_answer\"], \n",
    "            \"MiniGPT4\":model_name_res_dict[\"MiniGPT4\"][_id][\"model_answer\"], \n",
    "            \"XrayGPT\":model_name_res_dict[\"XrayGPT\"][_id][\"model_answer\"], \n",
    "            \"RadFM\":model_name_res_dict[\"RadFM\"][_id][\"model_answer\"]\n",
    "        }\n",
    "    except KeyError as e: # if key error, that means the GPT4o doesn't have result due the image privacy problem\n",
    "        print(e)\n",
    "        continue\n",
    "    if model_name_res_dict[\"GPT4o\"].__contains__(_id):\n",
    "        models_res[\"GPT4o\"] = model_name_res_dict[\"GPT4o\"][_id][\"model_answer\"]\n",
    "    for_eval = {\"structured_ground_truth\":gt, \"models\": models_res}\n",
    "    # print(for_eval)\n",
    "    example_ans = open_eval_chain.invoke(dict(input_pairs=for_eval))\n",
    "\n",
    "    final_evals.append(example_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "32994a83-0429-4b84-b0c2-ebd9e239660c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GPT4o': {'anatomy': {'recall_number': 0, 'wrong_number': 0},\n",
       "  'measurement': {'recall_number': 0, 'wrong_number': 0},\n",
       "  'symptom': {'recall_number': 0, 'wrong_number': 3}},\n",
       " 'CheXagent': {'anatomy': {'recall_number': 0, 'wrong_number': 0},\n",
       "  'measurement': {'recall_number': 0, 'wrong_number': 0},\n",
       "  'symptom': {'recall_number': 0, 'wrong_number': 0}},\n",
       " 'LLaVA_med': {'anatomy': {'recall_number': 0, 'wrong_number': 0},\n",
       "  'measurement': {'recall_number': 0, 'wrong_number': 0},\n",
       "  'symptom': {'recall_number': 0, 'wrong_number': 2}},\n",
       " 'LLaVA_med_15': {'anatomy': {'recall_number': 1, 'wrong_number': 2},\n",
       "  'measurement': {'recall_number': 0, 'wrong_number': 0},\n",
       "  'symptom': {'recall_number': 0, 'wrong_number': 3}},\n",
       " 'LLaVA': {'anatomy': {'recall_number': 0, 'wrong_number': 0},\n",
       "  'measurement': {'recall_number': 0, 'wrong_number': 0},\n",
       "  'symptom': {'recall_number': 0, 'wrong_number': 0}},\n",
       " 'LLaVA_13b': {'anatomy': {'recall_number': 0, 'wrong_number': 0},\n",
       "  'measurement': {'recall_number': 0, 'wrong_number': 0},\n",
       "  'symptom': {'recall_number': 0, 'wrong_number': 0}},\n",
       " 'LLM_CXR': {'anatomy': {'recall_number': 0, 'wrong_number': 0},\n",
       "  'measurement': {'recall_number': 0, 'wrong_number': 0},\n",
       "  'symptom': {'recall_number': 0, 'wrong_number': 0}},\n",
       " 'Med_flamingo': {'anatomy': {'recall_number': 1, 'wrong_number': 7},\n",
       "  'measurement': {'recall_number': 0, 'wrong_number': 0},\n",
       "  'symptom': {'recall_number': 1, 'wrong_number': 0}},\n",
       " 'MiniGPT4': {'anatomy': {'recall_number': 0, 'wrong_number': 1},\n",
       "  'measurement': {'recall_number': 0, 'wrong_number': 0},\n",
       "  'symptom': {'recall_number': 0, 'wrong_number': 4}},\n",
       " 'XrayGPT': {'anatomy': {'recall_number': 1, 'wrong_number': 2},\n",
       "  'measurement': {'recall_number': 0, 'wrong_number': 0},\n",
       "  'symptom': {'recall_number': 0, 'wrong_number': 1}},\n",
       " 'RadFM': {'anatomy': {'recall_number': 2, 'wrong_number': 11},\n",
       "  'measurement': {'recall_number': 0, 'wrong_number': 0},\n",
       "  'symptom': {'recall_number': 1, 'wrong_number': 0}}}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_evals[-1].dict() #samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "ad03f495-ebb5-479e-a2dc-f679c5fcbda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "res_dict = dict()\n",
    "for model in list(final_evals[0].dict().keys()):\n",
    "    res_dict[model] = {\"anatomy_recall\":0, \"symptom_recall\":0,  \"measurement_recall\":0, \"anatomy_error\":0, \"symptom_error\":0, \"measurement_error\":0}\n",
    "gt_anatomy_num, gt_symptom_num, gt_measure_num = 0, 0, 0\n",
    "for _id in question_ids:\n",
    "    gt = model_name_res_dict[\"LLM-CXR\"][_id][\"structured_ground_truth\"]\n",
    "    gt_anatomy_num += len(gt[\"anatomy\"])\n",
    "    gt_symptom_num += len(gt[\"symptom\"])\n",
    "    gt_measure_num += len(gt[\"measurement\"])\n",
    "    for model in list(final_evals[i].dict().keys()):\n",
    "        if final_evals[i].dict()[model] is not None:\n",
    "            res_dict[model]['anatomy_recall'] += final_evals[i].dict()[model]['anatomy']['recall_number']\n",
    "            res_dict[model]['anatomy_error'] += final_evals[i].dict()[model]['anatomy']['wrong_number']\n",
    "            res_dict[model]['symptom_recall'] += final_evals[i].dict()[model]['symptom']['recall_number']\n",
    "            res_dict[model]['symptom_error'] += final_evals[i].dict()[model]['symptom']['wrong_number']\n",
    "            res_dict[model]['measurement_recall'] += final_evals[i].dict()[model]['measurement']['recall_number']\n",
    "            res_dict[model]['measurement_error'] += final_evals[i].dict()[model]['measurement']['wrong_number']\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f3bdcc5-1dc7-41d3-83d5-408ef20ef9ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:58:28.110793500Z",
     "start_time": "2024-08-19T19:58:28.095973900Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the final evaluation results\n",
    "for k in list(res_dict.keys()):\n",
    "    rate_ana = res_dict[k]['anatomy_error'] / (res_dict[k]['anatomy_recall'] + res_dict[k]['anatomy_error'])\n",
    "    rate_sym = res_dict[k]['symptom_error'] / (res_dict[k]['symptom_recall'] + res_dict[k]['symptom_error'])\n",
    "    rate_mea = res_dict[k]['measurement_error'] / (res_dict[k]['measurement_recall'] + res_dict[k]['measurement_error'])\n",
    "    print(k, rate_ana, rate_sym, rate_mea)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
