{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aefd978c-2851-4494-b3a4-74d0a30e36a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#set the openai env variable here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68665ab4-caca-40ea-9691-f092030bf7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_version = \"2024-05-01-preview\"\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "model_kwargs = dict(\n",
    "    model=\"gpt-4-128k\", \n",
    "    azure_endpoint=\"endpoint here\",\n",
    "    api_key=\"key here\",\n",
    "    api_version=api_version,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "gpt_model = AzureChatOpenAI(**model_kwargs, cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef6c387f-089e-40ca-bb7e-154b1340746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Iterator, List, Optional, Sequence, Union\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b95e38de-d856-4ff8-9cf3-6c5b74eb2343",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg_eval = \"\"\"\n",
    "You are provided with a JSON object that includes a question-ground_truth pair and the corresponding answers from eleven different models. The question pertains to medical knowledge derived from an image. Your task is to evaluate the correctness of each model's answer, returning a binary output: 1 for correct and 0 for incorrect.\n",
    "\n",
    "The input format, a json object:\n",
    "{{\"question\":str, \"ground_truth\": str}, {\"models\":{\"model_1\": model_1_answer}, ...}}\n",
    "Your output format:\n",
    "{\n",
    "\"model_1\": int, 1 or 0,\n",
    "\"model_2\": int, 1 or 0,\n",
    "...\n",
    "}\n",
    "\n",
    "Instructions:\n",
    "- Consider an answer correct if it expresses the correct medical knowledge, even if the wording differs from the ground truth.\n",
    "- Aim to capture all possible correct expressions, not just those that match the ground truth verbatim.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c495eaa9-fa3f-4cf0-be92-62ac5fd95202",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Eval_ResList(BaseModel):\n",
    "    GPT4o: int = Field(description=\"The evaluation results of GPT4o.\", default=0)\n",
    "    CheXagent: int = Field(description=\"The evaluation results of CheXagent.\")\n",
    "    LLaVA_med: int = Field(description=\"The evaluation results of LLaVA-med.\")\n",
    "    LLaVA_med_15: int = Field(description=\"The evaluation results of LLaVA-med_15.\")\n",
    "    LLaVA: int = Field(description=\"The evaluation results of LLaVA.\")\n",
    "    LLaVA_13b: int = Field(description=\"The evaluation results of LLaVA_13b.\")\n",
    "    LLM_CXR: int = Field(description=\"The evaluation results of LLM-CXR.\")\n",
    "    Med_flamingo: int = Field(description=\"The evaluation results of Med-flamingo.\")\n",
    "    MiniGPT4: int = Field(description=\"The evaluation results of MiniGPT4.\")\n",
    "    XrayGPT: int = Field(description=\"The evaluation results of XrayGPT.\")\n",
    "    RadFM: int = Field(description=\"The evaluation results of RadFM.\")\n",
    "\n",
    "#Here we use the classic parser in LangChain: Pydantic, to ensure a strict parsing process\n",
    "eval_parser = PydanticOutputParser(pydantic_object=Eval_ResList)\n",
    "\n",
    "open_eval_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=system_msg_eval),\n",
    "    HumanMessagePromptTemplate(\n",
    "        prompt=PromptTemplate(\n",
    "            template=\"The question, ground_truth and models' model_response: {input_pairs}. Ensure that you read the model answers carefully and evaluate its correctness. Your evaluation output in a JSON object format, without extra explanation:\",\n",
    "            input_variables=[\"input_pairs\"],\n",
    "            partial_variables={\"format_instructions\": eval_parser.get_format_instructions()},\n",
    "        )\n",
    "    ),\n",
    "])\n",
    "\n",
    "open_eval_chain = (\n",
    "    open_eval_prompt \n",
    "    | gpt_model \n",
    "    | eval_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91449f9b-1757-4448-b859-adcec5d28122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get model inference results\n",
    "def load_model_ans(model_name):\n",
    "    ans_path = f\"/path/to/model/answer/{model_name}/mimic_open_res.jsonl\"\n",
    "    with open(ans_path, \"r\") as f:\n",
    "        results = [json.loads(line) for line in f]\n",
    "    formatted_results = []\n",
    "    id_results_dict = dict()\n",
    "    for i in results:\n",
    "        if i[\"question_type\"] == \"type4_Knowledge\":\n",
    "            _id = i[\"question_id\"]\n",
    "            new_dict = dict()\n",
    "            # print(i)\n",
    "            new_dict[\"ground_truth\"] = i[\"ground_truth\"]\n",
    "            new_dict[\"question\"] = i[\"prompt\"]\n",
    "            new_dict[\"model_answer\"] = i[\"model_answer\"]\n",
    "            id_results_dict[_id] = new_dict\n",
    "            formatted_results.append(new_dict)\n",
    "    return formatted_results, id_results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e790c48-4ea1-4343-bf2e-12c6f1814a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatted_results[:5]\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30414cf7-561b-4f78-b6f3-f9287dad8234",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_res_dict = dict()\n",
    "model_names = [\"LLM-CXR\", \"GPT4o\", \"llava-v1.6\", \"llava-v1.6-13b\", \"RadFM\", \"XrayGPT\", \"MiniGPT4\", \"Med-flamingo\", \"LLaVA-med-1.5\", \"LLaVA-med\", \"CheXagent\"]\n",
    "len(model_names)\n",
    "for model_name in model_names:\n",
    "    model_name_res_dict[model_name] = load_model_ans(model_name)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a0b74e1-eff8-4ad0-8504-e5974ed08e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_evals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86b015e4-58ec-4918-b262-fddc0a3923ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM-CXR 2318\n",
      "GPT4o 2252\n",
      "llava-v1.6 2318\n",
      "llava-v1.6-13b 2318\n",
      "RadFM 2318\n",
      "XrayGPT 2318\n",
      "MiniGPT4 2318\n",
      "Med-flamingo 2318\n",
      "LLaVA-med-1.5 2318\n",
      "LLaVA-med 2318\n",
      "CheXagent 2318\n"
     ]
    }
   ],
   "source": [
    "for k in model_name_res_dict:\n",
    "    print(k, len(model_name_res_dict[k])) # the number of all inferenced answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47f4d550-fc11-4443-b569-aef0a9b42e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_evals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b0c3d2c-e934-4e65-9b64-2d9e061a690f",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-08-19T20:11:46.279740100Z",
     "start_time": "2024-08-19T20:11:46.276585400Z"
    }
   },
   "outputs": [],
   "source": [
    "len(model_name_res_dict[\"LLM-CXR\"])\n",
    "question_ids = list(model_name_res_dict[\"LLM-CXR\"].keys())\n",
    "# question_ids\n",
    "for _id in tqdm(question_ids[500:]):\n",
    "    gt = model_name_res_dict[\"LLM-CXR\"][_id][\"ground_truth\"]\n",
    "    q = model_name_res_dict[\"LLM-CXR\"][_id][\"question\"]\n",
    "    models_res = {\n",
    "        \"CheXagent\":model_name_res_dict[\"CheXagent\"][_id][\"model_answer\"], \n",
    "        \"LLaVA_med\":model_name_res_dict[\"LLaVA-med\"][_id][\"model_answer\"], \n",
    "        \"LLaVA_med_15\":model_name_res_dict[\"LLaVA-med-1.5\"][_id][\"model_answer\"], \n",
    "        \"LLaVA\":model_name_res_dict[\"llava-v1.6\"][_id][\"model_answer\"], \n",
    "        \"LLaVA_13b\":model_name_res_dict[\"llava-v1.6-13b\"][_id][\"model_answer\"], \n",
    "        \"LLM_CXR\":model_name_res_dict[\"LLM-CXR\"][_id][\"model_answer\"], \n",
    "        \"Med_flamingo\":model_name_res_dict[\"Med-flamingo\"][_id][\"model_answer\"], \n",
    "        \"MiniGPT4\":model_name_res_dict[\"MiniGPT4\"][_id][\"model_answer\"], \n",
    "        \"XrayGPT\":model_name_res_dict[\"XrayGPT\"][_id][\"model_answer\"], \n",
    "        \"RadFM\":model_name_res_dict[\"RadFM\"][_id][\"model_answer\"]\n",
    "    }\n",
    "    if model_name_res_dict[\"GPT4o\"].__contains__(_id):\n",
    "        models_res[\"GPT4o\"] = model_name_res_dict[\"GPT4o\"][_id][\"model_answer\"]\n",
    "    for_eval = {\"question\":q, \"ground_truth\":gt, \"models\": models_res}\n",
    "    # print(for_eval)\n",
    "    example_ans = open_eval_chain.invoke(dict(gr_pairs=for_eval))\n",
    "\n",
    "    final_evals.append(example_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "32994a83-0429-4b84-b0c2-ebd9e239660c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GPT4o': 1,\n",
       " 'CheXagent': 1,\n",
       " 'LLaVA_med': 1,\n",
       " 'LLaVA_med_15': 1,\n",
       " 'LLaVA': 0,\n",
       " 'LLaVA_13b': 1,\n",
       " 'LLM_CXR': 1,\n",
       " 'Med_flamingo': 1,\n",
       " 'MiniGPT4': 0,\n",
       " 'XrayGPT': 0,\n",
       " 'RadFM': 1}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_evals[-5].dict() #example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4d12e4d6-4f08-4e8f-845a-707929916b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = final_evals[0].dict()\n",
    "for i in final_evals[1:]:\n",
    "    for k in i.dict():\n",
    "        all_models[k] += i.dict()[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad446c0e-cdb8-436d-8e61-6e48d521aa61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T20:12:20.054877700Z",
     "start_time": "2024-08-19T20:12:20.041413900Z"
    }
   },
   "outputs": [],
   "source": [
    "#the evaluation results\n",
    "for i in all_models:\n",
    "    num = all_models[i]\n",
    "    print(i, (2318-num)/2318) #2318 is the number of all questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05106571936056838"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2252 - 2137) / 2252 # For GPT-4o, it can not answer some of the questions, so the total number is less"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "083a713e-247d-4d72-a414-9b4f819cac74"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
